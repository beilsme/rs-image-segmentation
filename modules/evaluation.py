#!/usr/bin/env python3.12
# -*- coding: utf-8 -*-
'''
å›¾åƒåˆ†å‰² - ç›‘ç£åˆ†ç±»è¯„ä¼°æ¨¡å—

åŠŸèƒ½ï¼šè®¡ç®—æ··æ·†çŸ©é˜µã€æ€»ä½“ç²¾åº¦å’Œ Kappa ç³»æ•°ï¼Œå¹¶ç”Ÿæˆæ··æ·†çŸ©é˜µå¯è§†åŒ–å›¾

è¾“å…¥ï¼š  
  - é¢„æµ‹ç»“æœæ•°ç»„ï¼ˆNumPy .npy æ ¼å¼ï¼‰  
  - çœŸå®æ ‡ç­¾æ•°ç»„ï¼ˆNumPy .npy æ ¼å¼ï¼‰

è¾“å‡ºï¼š  
  - æ··æ·†çŸ©é˜µå›¾åƒï¼ˆPNG æ ¼å¼ï¼‰  
  - æ§åˆ¶å°æ‰“å°åˆ†ç±»æŠ¥å‘Šä¸ç»Ÿè®¡ç»“æœ  
  - è¿”å›åŒ…å«è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸

ä½œè€…ï¼šMeng Yinan  
æ—¥æœŸï¼š2025-05-26
'''


import os
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    confusion_matrix, classification_report,
    cohen_kappa_score, accuracy_score
)
from utils.set_chinese_font import set_chinese_font

def evaluate_classification(prediction, ground_truth, class_names, save_dir="output/supervised/evaluation"):
    os.makedirs(save_dir, exist_ok=True)

    # Flatten for sklearn
    y_pred = prediction.flatten()
    y_true = ground_truth.flatten()

    # è¿‡æ»¤æ‰æœªæ ‡æ³¨åŒºåŸŸï¼ˆ0ï¼‰
    valid_mask = y_true > 0
    y_true = y_true[valid_mask]
    y_pred = y_pred[valid_mask]

    # å‡†å¤‡ labels åˆ—è¡¨ï¼Œä¿è¯ä¸ class_names ä¸€ä¸€å¯¹åº”
    labels = list(range(1, len(class_names) + 1))

    # è®¡ç®—æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_true, y_pred, labels=labels)
    oa = accuracy_score(y_true, y_pred)
    kappa = cohen_kappa_score(y_true, y_pred)

    # æ‰“å°æŠ¥å‘Šï¼Œæ˜¾å¼ä¼ å…¥ labels
    print("ğŸ” åˆ†ç±»æŠ¥å‘Šï¼š")
    print(classification_report(
        y_true,
        y_pred,
        labels=labels,
        target_names=class_names,
        digits=3
    ))
    print(f"âœ… æ€»ä½“ç²¾åº¦ï¼ˆOAï¼‰: {oa:.3f}")
    print(f"âœ… Kappa ç³»æ•°: {kappa:.3f}")

    # ç»˜å›¾
    set_chinese_font()
    plt.figure(figsize=(8, 6))
    sns.heatmap(
        cm, annot=True, fmt="d", cmap="YlGnBu",
        xticklabels=class_names,
        yticklabels=class_names
    )
    plt.title("åˆ†ç±»ç»“æœæ··æ·†çŸ©é˜µ")
    plt.xlabel("é¢„æµ‹æ ‡ç­¾")
    plt.ylabel("çœŸå®æ ‡ç­¾")
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, "confusion_matrix.png"))
    plt.close()
    print("ğŸ“Š æ··æ·†çŸ©é˜µå·²ä¿å­˜")

    return {
        "confusion_matrix": cm,
        "overall_accuracy": oa,
        "kappa": kappa
    }


if __name__ == '__main__':
    # â€”â€” é…ç½® â€”â€” #
    # é¢„æµ‹ç»“æœï¼ˆHÃ—W æ•´æ•°æ•°ç»„ï¼Œ1,2,3â€¦ä»£è¡¨å„ç±»ï¼‰
    pred_path = "../output/feature_outputs/all_hierarchical_features.npy"
    # çœŸå€¼ï¼ˆHÃ—W æ•´æ•°æ•°ç»„ï¼Œ0ä»£è¡¨æœªæ ‡æ³¨ï¼Œå…¶ä½™ 1,2,3â€¦å¯¹åº”ç±»åˆ«ï¼‰
    gt_path   = "../output/ROI/roi_mask.npy"
    # ç±»åˆ«åç§°åˆ—è¡¨ï¼Œé¡ºåºè¦å’Œ 1,2,3â€¦ å¯¹åº”
    class_names = ["æ°´ä½“", "æ¤è¢«", "å»ºè®¾ç”¨åœ°"]

    # â€”â€” åŠ è½½ â€”â€” #
    prediction   = np.load(pred_path)
    ground_truth = np.load(gt_path)

    # â€”â€” æ£€æŸ¥å°ºå¯¸åŒ¹é… â€”â€” #
    if prediction.shape != ground_truth.shape:
        raise ValueError(
            f"å°ºå¯¸ä¸åŒ¹é…: prediction{prediction.shape} vs ground_truth{ground_truth.shape}"
        )

    # â€”â€” è¯„ä¼° â€”â€” #
    results = evaluate_classification(
        prediction,
        ground_truth,
        class_names,
        save_dir="../output/supervised/evaluation"
    )

    # â€”â€” æ‰“å° summary â€”â€” #
    print("\n=== è¯„ä¼°ç»“æœæ‘˜è¦ ===")
    print(f"Overall Accuracy: {results['overall_accuracy']:.3f}")
    print(f"Kappa Coefficient: {results['kappa']:.3f}")
    print("====================")